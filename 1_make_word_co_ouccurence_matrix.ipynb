{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create word co-occurence matrix from raw files\n",
    "\n",
    "This notebook constructs the word co-occurence matrix from the raw Supreme Curt opinions from [CourtListener](https://www.courtlistener.com/). To download the raw data onto your computer\n",
    "\n",
    "- click this link to download the opinions: https://www.courtlistener.com/api/bulk-data/opinions/scotus.tar.gz (**warning** this will download a large file)\n",
    "\n",
    "- unzip the folder and set `sentence_dir` below to the directory containing the .json files\n",
    "\n",
    "CourtListener has much more data which you can learn about from https://www.courtlistener.com/api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you need to modify this!\n",
    "\n",
    "# where to read the opinion file from\n",
    "op_dir = '/Users/iaincarmichael/data/word_embed/scotus/opinions/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from itertools import combinations, chain\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# import local code\n",
    "import sys, os\n",
    "sys.path.append(os.getcwd() + '/code/')\n",
    "from save import save_matrix, save_vocabulary\n",
    "from courtlistener import json_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_files = glob.glob(op_dir + \"*.json\")\n",
    "\n",
    "# select a subset of the text files to process \n",
    "# this makes things go faster -- comment out if you want to process all the text files\n",
    "json_files = np.random.choice(json_files, size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create word co-occureance matrix from documents\n",
    "\n",
    "Processes each supreme court opinion\n",
    "\n",
    "- lower case words\n",
    "- remove \\n characters (new lines)\n",
    "- remove punctuation\n",
    "- remove sentences fewer than 5 characters\n",
    "- tokenize document into sentences\n",
    "- todkenize sentences into words\n",
    "\n",
    "then counts number of times word co-occur in the same sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def docs2sentences_word_coo(json_files):\n",
    "    \"\"\"\n",
    "    Creates the word co-occurence matrix counting the number of times each word occurs in the same sentences\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    json_files: list of paths to raw opinion json files\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    co_counts: sparse (csr_matrix) counting the number of times pairs of words co-occur in the same sentence\n",
    "    \n",
    "    vocab: a list of the vocabulary in giving the row/column order for co_counts\n",
    "    \n",
    "    word_counts: list counting the number of times each word appears in the indexed by vocab\n",
    "    \"\"\"\n",
    "\n",
    "    # use to remove punctuation from text\n",
    "    kill_punct = dict((ord(char), None) for char in string.punctuation)\n",
    "    \n",
    "    word_counter = Counter() # single words\n",
    "    pair_counts = Counter() # word pairs\n",
    "    \n",
    "\n",
    "    for f in json_files:\n",
    "\n",
    "        # get opinion text\n",
    "        text = json2text(f)\n",
    "\n",
    "        # process opinion text and tokenize sentences into words\n",
    "        sentences_word_tok = text2tok_sentences(text, kill_punct)\n",
    "        \n",
    "        # count numer of times each word appears\n",
    "        word_counter.update(chain(*sentences_word_tok))\n",
    "\n",
    "        # Get a list of all of the combinations\n",
    "        extended = [tuple(combinations(s, 2)) for s in sentences_word_tok]\n",
    "        extended = chain(*extended)\n",
    "\n",
    "        # Sort the combinations so that A,B and B,A are treated the same\n",
    "        extended = [tuple(sorted(d)) for d in extended]\n",
    "\n",
    "        # count the combinations\n",
    "        pair_counts.update(extended)\n",
    "\n",
    "    # get vocabulary\n",
    "    vocab = zip(*pair_counts.keys())\n",
    "    vocab = list(set(vocab[0]).union(set(vocab[1])))\n",
    "    w2i = {vocab[i]: i for i in range(len(vocab))}\n",
    "\n",
    "    # construct counts as lil matrix but return them as csr_matrix\n",
    "    co_counts = lil_matrix((len(vocab), len(vocab)))\n",
    "    for p, c in pair_counts.iteritems():\n",
    "        co_counts[w2i[p[0]], w2i[p[1]]] = c\n",
    "\n",
    "    co_counts = (co_counts + co_counts.T).tocsr()    \n",
    "    \n",
    "    return co_counts, vocab, [word_counter[vocab[i]] for i in range(len(vocab))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def json2text(path):\n",
    "    \"\"\"\n",
    "    Given a path to a json opinion file from CourtListener, returns the text of the opinion\n",
    "    \"\"\"\n",
    "\n",
    "    # read json file, parse html and get the text\n",
    "    html = BeautifulSoup(json_to_dict(path)['html_with_citations'], 'lxml')\n",
    "\n",
    "    return html.get_text()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text2tok_sentences(text, char_map={}):\n",
    "    \"\"\"\n",
    "    Processes and tokenizes a document\n",
    "    \n",
    "    - lower case words\n",
    "    - tokenize into sentences\n",
    "    - remove \\n\n",
    "    - remove punctuation\n",
    "    - remove sentences fewer than 5 charaters\n",
    "    - tokenize sentences into words\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: the document as a string\n",
    "    char_map: a dict mapping characters with s.translate(char_map) e.g. \n",
    "    can be used to remove punctuation\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    A list of lists\n",
    "    Outer list is for sentences\n",
    "    Inner list is for each word in the sentence\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # lowercase text\n",
    "    text = text.lower()\n",
    "\n",
    "    # tokenize text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # remove \\n characters\n",
    "    # remove sentences with fewer than 5 character\n",
    "    # remove punctuation\n",
    "    sentences = [s.strip('\\n').translate(char_map) for s in sentences if len(s) >= 5]\n",
    "\n",
    "\n",
    "    return [[w for w in word_tokenize(s)] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "co_counts, vocab, word_counts = docs2sentences_word_coo(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the data!\n",
    "save_matrix('data/co_counts_small_ex', co_counts)\n",
    "\n",
    "vocab = [w.encode('utf-8') for w in vocab] # annoying unicode hack\n",
    "save_vocabulary('data/vocab_small_ex.txt', vocab)\n",
    "\n",
    "np.save('data/word_counts_small_ex', word_counts)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
