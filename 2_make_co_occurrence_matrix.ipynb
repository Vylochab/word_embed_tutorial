{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute word co-occurence matrix\n",
    "\n",
    "This notebook constructs the word co-occurence matrix from a bag of sentences file. In particular, given a file with one sentence on each line we create a matrix **counts** which counts the number of times each pair of words in our vocabulary co-occur within the same sentence.\n",
    "\n",
    "\n",
    "**code details**\n",
    "\n",
    "The code uses the nltk package for text processing (primarily tokenizing sentences into words). nltk is both a software package and (free) textbook -- you can read more here: http://www.nltk.org/book/\n",
    "\n",
    "The word co-occurence matrix is a very large, sparse matrix (i.e. most words do not co-occur with most other words). There are efficient data structures to handle very large sparse matrices to make working with them tractable on a laptop; we use scipy's sparse matrix libary (https://docs.scipy.org/doc/scipy/reference/sparse.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import combinations, chain, islice\n",
    "\n",
    "from nltk.tokenize import  word_tokenize\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "\n",
    "# import local code files\n",
    "import sys, os\n",
    "sys.path.append(os.getcwd() + '/code/')\n",
    "\n",
    "from save import save_vocabulary, save_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sentences_file` should be a file with one sentence on each line. Note the sentence file has been processed already (e.g. documents tokenized into sentences, words lower cased, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you should modify this path on your own computer\n",
    "sentences_file = '/Users/iaincarmichael/data/word_embed/scotus/sentences/scotus_sentences_1e4.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentences2counts(sentences_file, N=1000):\n",
    "    \"\"\"\n",
    "    Counts the number of times words co-occur with each other in a sentence.\n",
    "    Reads in the bag of sentence file N lines at at time to reduce the memory usage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences_file: path to bag of sentences file\n",
    "    N: how many lines to read in at a time\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    counts\n",
    "    vocab\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pair_counts = Counter()\n",
    "    with open(sentences_file) as f:\n",
    "        \n",
    "        # go through file N lines at at time\n",
    "        while True:\n",
    "            sentences = list(islice(f, N)) # read in N lines\n",
    "\n",
    "            if sentences:\n",
    "                # tokenize sentences into words\n",
    "                sentences_word_tok =  [[w for w in word_tokenize(s)] for s in sentences]\n",
    "\n",
    "                # Get a list of all of the combinations\n",
    "                extended = [tuple(combinations(s, 2)) for s in sentences_word_tok]\n",
    "                extended = chain(*extended)\n",
    "\n",
    "                # Sort the combinations so that A,B and B,A are treated the same\n",
    "                extended = [tuple(sorted(d)) for d in extended]\n",
    "\n",
    "                # count the combinations\n",
    "                pair_counts.update(extended)\n",
    "    \n",
    "            else: # stop while loop if we are at the end of the file\n",
    "                break\n",
    "                \n",
    "                \n",
    "    # get vocabulary\n",
    "    vocab = zip(*pair_counts.keys())\n",
    "    vocab = list(set(vocab[0]).union(set(vocab[1])))\n",
    "    w2i = {vocab[i]: i for i in range(len(vocab))}\n",
    "    \n",
    "    # construct counts as lil matrix but return them as csr_matrix\n",
    "    counts = lil_matrix((len(vocab), len(vocab)))\n",
    "    for c, p in enumerate(pair_counts):\n",
    "        counts[w2i[p[0]], w2i[p[1]]] = c\n",
    "        \n",
    "    counts = (counts + counts.T).tocsr()\n",
    "    \n",
    "    return counts, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "counts, vocab = sentences2counts(sentences_file, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** save the word co-occurence counts and the vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the vocabulary and the counts matrix\n",
    "save_vocabulary('data/vocab_1e4.txt', vocab)\n",
    "save_matrix('data/scotus_counts_1e4', counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
