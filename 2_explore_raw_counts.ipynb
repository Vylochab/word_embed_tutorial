{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore word co-occurence counts\n",
    "\n",
    "This notebook explores the raw word co-occurence counts from a corpus. Background material for this notebook can be found in https://web.stanford.edu/~jurafsky/slp3/15.pdf. \n",
    "\n",
    "The notebook loads a pre-computed word co-occurence matrix and vocabulary and some some initial exploration e.g. look at word count statistics. \n",
    "\n",
    "Note that the word co-occurence matrix, `counts` is a scipy sparse matrix which which works a little differently than numpy matrices -- for further documentaiton see https://docs.scipy.org/doc/scipy/reference/sparse.html.\n",
    "\n",
    "## data\n",
    "\n",
    "The github repo comes with a small data set (from a random sample of 1000 court cases). You can download larger data files from https://drive.google.com/open?id=0B40b05f-8LWtVGsybWw4OTVyV00 then place them in the data/ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# import local code files\n",
    "import sys, os\n",
    "sys.path.append(os.getcwd() + '/code/')\n",
    "\n",
    "from save import load_vocabulary, load_matrix\n",
    "from explore_counts_fun import top_counts_bar_plot\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load word co-occurence matrix and vocabulary**\n",
    "\n",
    "- `co_counts` is scipy sparse matrix (see https://docs.scipy.org/doc/scipy/reference/sparse.html).\n",
    "- `w2i` is a dictionary mappting words to a unique index matching the rows of `counts`\n",
    "- `i2w` is a list mapping indices to words\n",
    "- `word_counts` is a dict displaying the number of times each word each word appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i, i2w = load_vocabulary('data/vocab_small_ex.txt')\n",
    "co_counts = load_matrix('data/co_counts_small_ex')\n",
    "word_counts = np.load('data/word_counts_small_ex.npy')\n",
    "\n",
    "# uncomment this code if you have the larger data file\n",
    "# w2i, i2w = load_vocabulary('data/vocab_10000.txt')\n",
    "# co_counts = load_matrix('data/co_counts_10000')\n",
    "# word_counts = np.load('data/word_counts_10000.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6d114d3f87ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lawyer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi2w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m17743\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(w2i['lawyer'])\n",
    "print(i2w[17743])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(co_counts[w2i['lawyers'], w2i['criminal']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore word occurence counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "heapq.nlargest(N, zip(word_counts, i2w)) # this piece of code finds the largest values of total_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_counts_bar_plot(word_counts,\n",
    "                    i2w,\n",
    "                    N=20,\n",
    "                    title='',\n",
    "                    figsize=[10, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# co-occurence statistics for a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = 'lawyer'\n",
    "\n",
    "# vector of co-occurence counts for word\n",
    "# the .toarray().reshape(-1) converts the row vector to a numpy array\n",
    "word_co_counts = co_counts[w2i[word], :].toarray().reshape(-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_counts_bar_plot(word_co_counts,\n",
    "                    i2w,\n",
    "                    N=20,\n",
    "                    title='top words co-occuring with %s' % word,\n",
    "                    figsize=[10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=[10, 10])\n",
    "# plt.hist(word_coo_counts, bins=1000)#np.arange(max(word_coo_counts)));\n",
    "# plt.xlim([0, max(word_coo_counts)])\n",
    "# plt.xlabel('counts')\n",
    "# plt.title('histogram of co-occurence couts for all words with %s'% word)\n",
    "\n",
    "\n",
    "# print 'mean: %f' % np.mean(word_coo_counts)\n",
    "# print 'var: %f' % np.var(word_coo_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word similarity\n",
    "\n",
    "Given the word co-occurence statistics, we can compute similarities between two words using a number of measures (e.g. see section 15.3 https://web.stanford.edu/~jurafsky/slp3/15.pdf). \n",
    "\n",
    "\n",
    "The code below is also in word_similarity.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(word1, word2, sim='angle'):\n",
    "    \"\"\"\n",
    "    Computes the similarity between two words\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word1, word2: words to compare\n",
    "    sim: which similarity measure to use (angle, cosine, jaccard, dice)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    similarity measure between two words\n",
    "    \"\"\"\n",
    "    \n",
    "    v1 = vec(word1)\n",
    "    v2 = vec(word2)\n",
    "\n",
    "    if sim == 'angle':\n",
    "        return angle_between(v1, v2)\n",
    "    elif sim == 'cosine':\n",
    "        return cosine_sim(v1, v2)\n",
    "    elif sim == 'jaccard':\n",
    "        return jaccard_sim(v1, v2)\n",
    "    elif sim == 'dice':\n",
    "        return dice_sim(v1, v2)\n",
    "    else:\n",
    "        raise ValueError('sim must be one of: angle, cosine, jaccard, dice')\n",
    "\n",
    "def vec(word):\n",
    "    \"\"\"\n",
    "    Returns the vector for word as an array\n",
    "    \"\"\"\n",
    "    return co_counts[w2i[word], :].toarray().reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** here are a few common similarity functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(v, w):\n",
    "    return np.dot(v, w) / np.sqrt(np.dot(v, v) * np.dot(w, w))\n",
    "\n",
    "def angle_between(v, w):\n",
    "    cos_angle = cosine_sim(v, w)\n",
    "    angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def jaccard_sim(v, w):\n",
    "    return np.minimum(v, w).sum()/np.maximum(v, w).sum()\n",
    "\n",
    "def dice_sim(v, w):\n",
    "     return 2.0 * np.minimum(v, w).sum() /(v + w).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'lawyer'\n",
    "word2 = 'lawyers'\n",
    "\n",
    "print('similarity between %s and %s' % (word1, word2))\n",
    "print()\n",
    "print('angle: %f' % similarity(word1, word2, sim='angle'))\n",
    "print('cosine: %f' % similarity(word1, word2, sim='cosine'))\n",
    "print('jaccard: %f' % similarity(word1, word2, sim='jaccard'))\n",
    "print('dice: %f' % similarity(word1, word2, sim='dice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
