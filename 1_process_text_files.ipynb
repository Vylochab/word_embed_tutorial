{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process raw text files\n",
    "\n",
    "The code in this notebook processes raw textfiles. The first section converts a set of documents into a bag of sentences file -- i.e. M documents to one file with one sentence per line. The second section is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.getcwd() + '/code/')\n",
    "from courtlistener import json_to_dict\n",
    "\n",
    "# you need to modify these!\n",
    "op_dir = '/Users/iaincarmichael/data/word_embed/scotus/opinions/' # where to read the opinion file from\n",
    "sentence_dir = '/Users/iaincarmichael/data/word_embed/scotus/sentences/' # where to write the sentence file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# # select a subset of the text files to process \n",
    "# # this makes things go faster -- comment out if you want to process all the text files\n",
    "json_files = glob.glob(op_dir + \"*.json\")\n",
    "json_files = np.random.choice(json_files, size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert files to bag of sentences\n",
    "\n",
    "takes all SCOTUS text files, does some light preprocessing then writes them to a single file with one sentence on each line\n",
    "\n",
    "- lower case words\n",
    "- tokenize into sentences using nltk\n",
    "- remove \\n characters (new lines)\n",
    "- remove punctuation\n",
    "- remove sentences fewer than 5 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 548 ms, total: 1min 5s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# use to remove punctuation from text\n",
    "kill_punct = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "with open(sentence_dir + 'scotus_sentences_5000.txt','ab') as write_file:\n",
    "    for f in json_files:\n",
    "\n",
    "        # read json file, parse html and get the text\n",
    "        text = BeautifulSoup(json_to_dict(f)['html_with_citations'], 'lxml').get_text()\n",
    "\n",
    "        # lowercase text\n",
    "        text = text.lower()\n",
    "\n",
    "        # tokenize text into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # remove \\n characters\n",
    "        # remove sentences with fewer than 5 character\n",
    "        # remove punctuation\n",
    "        sentences = [s.strip('\\n').translate(kill_punct) for s in sentences if len(s) >= 5]\n",
    "\n",
    "        # write file with one sentence on each line\n",
    "        for s in sentences:\n",
    "            write_file.write(s.encode('utf-8'))\n",
    "            write_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean files (optional)\n",
    "\n",
    "This section (optional) processes each document and creates a new, processed document i.e. M documents to M documents. This could be useful for making bag of words/TF-IDF matrix or for window contexts\n",
    "\n",
    "- lower case all words\n",
    "- remove punctuation\n",
    "- some light preprocessing with word_tokenize()\n",
    "- write a new file with cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_text_dir = '/Users/iaincarmichael/data/word_embed/scotus/processed_text_files/'\n",
    "\n",
    "# code is commented so you don't accidently run something that will eat a lot of time/memory!\n",
    "\n",
    "# for tf in raw_text_files:\n",
    "#     # read in document from raw text file\n",
    "#     text = open(tf).read().decode(\"utf8\")\n",
    "\n",
    "#     # lowercase text\n",
    "#     text = text.lower()\n",
    "\n",
    "#     # remove punctuation\n",
    "#     text = text.translate(kill_punct)\n",
    "\n",
    "#     # this removes a lot of annoying stuff\n",
    "#     words = word_tokenize(text)\n",
    "#     text = ' '.join(words)\n",
    "\n",
    "#     op_id = tf.split('/')[-1].split('.txt')[0]\n",
    "#     with open(processed_text_dir + '%s.txt' % op_id,'ab') as wf:\n",
    "#         wf.write(text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
