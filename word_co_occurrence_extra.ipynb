{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute word co-occurence matrix (extra material)\n",
    "\n",
    "Below are some examples of other choices we might have made to create the word co-occurence matrix. These are not strictly necessary (or even a good idea), but are meant to give a taste for the plethora of choices we have to make while processing text. They also might be useful example code. \n",
    "\n",
    "Feel free to skip the following sections upon first reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import  word_tokenize\n",
    "\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# Iain's code\n",
    "from save import save_vocabulary, load_vocabulary, save_matrix, load_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 272279 sentences \n"
     ]
    }
   ],
   "source": [
    "sentences_file = '/Users/iaincarmichael/data/word_embed/scotus/sentences/scotus_sentences.txt'\n",
    "\n",
    "# load senteces from bag of sentences file\n",
    "with open(sentences_file) as f:\n",
    "    sentences = [line.strip('\\n') for line in f.readlines()]\n",
    "\n",
    "print 'loaded %d sentences ' % len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sentences2word_tok(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize sentences into words using nltk's word_tokenize.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences: a list of sentences\n",
    "    \n",
    "    Output\n",
    "    -------\n",
    "    A list of lists. The outer list has the same length as sentences. The inner lists have one word token per entry\n",
    "    \"\"\"\n",
    "    return [[w for w in word_tokenize(s)] for s in sentences]\n",
    "\n",
    "def sentences2counts(sentences_word_tok):\n",
    "    \"\"\"\n",
    "    Compute the matrix of word co-occurances.\n",
    "  \n",
    "    Some code borrowed from from https://stackoverflow.com/questions/42814452/co-occurrence-matrix-from-list-of-words-in-python\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences_word_tok: list of tokenized sentences\n",
    "    w2i: dictionary mapping words to indices\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    symmetric, csr matrix with word co-occurence counts\n",
    "    \"\"\"\n",
    "    # create vocabulary set\n",
    "    vocab = set()\n",
    "    for s in sentences_word_tok:\n",
    "        vocab = vocab.union(set(s))\n",
    "    vocab = list(vocab)\n",
    "    \n",
    "    # dict mapping words to indices\n",
    "    w2i = {vocab[i]: for i in range(len(vocab))}\n",
    "    \n",
    "    # Get a list of all of the combinations\n",
    "    expanded = [tuple(itertools.combinations(s, 2)) for s in sentences_word_tok]\n",
    "    expanded = itertools.chain(*expanded)\n",
    "\n",
    "    # Sort the combinations so that A,B and B,A are treated the same\n",
    "    expanded = [tuple(sorted(d)) for d in expanded]\n",
    "\n",
    "    # count the combinations\n",
    "    pair_counts = Counter(expanded)\n",
    "\n",
    "    # construct counts as lil matrix but return them as csr_matrix\n",
    "    counts = lil_matrix((len(w2i), len(w2i)))\n",
    "    for p in pair_counts:\n",
    "        counts[w2i[p[0]], w2i[p[1]]] = pair_counts[p]\n",
    "\n",
    "    return (counts + counts.T).tocsr(), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# w2i, i2w = vocab_from_sentences(sentences)\n",
    "# sentences_word_tok = sentences2word_tok(sentences)\n",
    "# counts = sentences2counts(sentences_word_tok, w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence lengths\n",
    "\n",
    "Text data is often messy and we make a lot of assumptions when we process it which will be incorrect some portion of the time (e.g. the sentence tokenizer will fail sometimes). It's a good idea to explore the data and frequently perform sanity checks using basic statistics. \n",
    "\n",
    "Below we plot a histogram of sentence lenths (in this case lengh = number of characters). The histogram shows us there is one very long sentence which is likely a failure of the sentence tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sent_lengths = [len(s) for s in sentences]\n",
    "plt.hist(sent_lengths, bins=100);\n",
    "plt.xlabel('sentence lengths')\n",
    "plt.ylabel('count')\n",
    "plt.title('histogram of sentence lengths')\n",
    "\n",
    "print 'longest sentence is %d characters' % max(sent_lengths)\n",
    "\n",
    "# uncommet to view the very long sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences[np.argmax(sent_lengths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at commonly occuring words and word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of all words which appear in the file\n",
    "word_list = []\n",
    "for s in sentences:\n",
    "    word_list.extend(word_tokenize(s))\n",
    "\n",
    "# counts the number of times each word appears in the file\n",
    "word_counts = Counter(word_list)\n",
    "\n",
    "\n",
    "print 'there are %d unique words' % len(word_counts.keys())\n",
    "print ''\n",
    "w = 'constitution'\n",
    "print '%s appears %d times' % (w, word_counts[w])\n",
    "w = 'certiorari'\n",
    "print '%s appears %d times' % (w, word_counts[w])\n",
    "\n",
    "\n",
    "# show the most commonly occuring words\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alternate word tokenization\n",
    "\n",
    "Given a sentence `s`, the easiest way to tokenize it into words is to break it up by spaces i.e.\n",
    "\n",
    "```python\n",
    "s.split(' ')\n",
    "```\n",
    "\n",
    "We, however, are using nltk's `word_tokenize` function\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(s)\n",
    "```\n",
    "\n",
    "The two are pretty similary, but will sometimes give different results. You can explore the difference below.\n",
    "\n",
    "In addition to nltk and `split`, there are many other word tokenizers (e.g. [Stanford's CoreNLP package](https://github.com/Lynten/stanford-corenlp)) and you can make your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find indices where split and word_tokenize don't agree\n",
    "[i for i in range(100) if len(sentences[i].split(\" \")) !=  len(word_tokenize(sentences[i]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = sentences[52]\n",
    "\n",
    "print 'sentence:'\n",
    "print s\n",
    "print\n",
    "print 'using s.split(' ')'\n",
    "print s.split(\" \")\n",
    "print\n",
    "print 'using word_tokenize from nltk'\n",
    "print word_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bi-grams\n",
    "\n",
    "Bi-grams are ordered pairs of words i.e `the_quick`, `quick_brown`. We currently are currently using single words as tokens, but we might want to use bi-grams (or higher order n-grams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "# we could count bi-grams as follows, but we won't pursue this for now\n",
    "ex_sentence = 'the quick brown fox jumped over the lazy dog'\n",
    "\n",
    "print ex_sentence\n",
    "print\n",
    "print list(bigrams(word_tokenize(ex_sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### restrict vocabulary as part of pre-processing\n",
    "\n",
    "We currently use every word that appears in our sentences. We might want to restrict ourselves to the K (e.g. 1000, 10,000, etc) most commonly ocuring words as a part of pre-processing using only two minor modifications.\n",
    "\n",
    "\n",
    "This line finds the 1000 most commonly occuring words\n",
    "```python\n",
    "word_counts.most_common()[:1000]\n",
    "```\n",
    "\n",
    "Given an vocabulary set (e.g. the most commonly occuring words) this line removes all non-vocabulary words\n",
    "```python\n",
    "[[w for w in word_tokenize(s) if w in vocab] for s in sentences]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_words = word_counts.most_common()[:1000]\n",
    "\n",
    "# new vocabulary\n",
    "vocab = set([cw[0] for cw in common_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove words that are not in the vocabulary while tokeinzing into sentences\n",
    "sentences_word_tok = [[w for w in word_tokenize(s) if w in vocab] for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove short sentences\n",
    "\n",
    "We can remove very short sentences (e.g. those with 2 or fewer words) as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove sentences that have less than three words\n",
    "sentences_word_tok = [s for s in sentences_word_tok if len(s) >= 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming\n",
    "\n",
    "Stemming converts words into their base form in some sense (read more about it here http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization). For example, we might not want to distinguish between **hat** and **hats**. Stemming would convert **hats** into **hat**.\n",
    "\n",
    "We could have stemmed all of our words as a first pre-processing step. nltk provides a number of stemmers including the Porter and Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "s = sentences_word_tok[10]\n",
    "\n",
    "print s\n",
    "print\n",
    "port_stemmer = PorterStemmer()\n",
    "print [port_stemmer.stem(w) for w in s]\n",
    "\n",
    "sb_stemmer = SnowballStemmer(language='english')\n",
    "print [sb_stemmer.stem(w) for w in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive sentence to counts\n",
    "\n",
    "The most straightforward way to count word co-occurences within a sentence is with a double for loop (see below). While the code is easier to understand, than the above `sentences2counts()`, it runs much slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences2counts_naive(sentences_word_tok, w2i):\n",
    "    \"\"\"\n",
    "    Compute the matrix of word co-occurances.\n",
    "    This is the most simple way of counting co-occurences, but will take a long time to run\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences_word_tok: list of tokenized sentences\n",
    "    w2i: dictionary mapping words to indices\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    symmetric, csr matrix with word co-occurence counts\n",
    "    \"\"\"\n",
    "\n",
    "    # construct counts as lil matrix but return them as csr_matrix\n",
    "    counts = lil_matrix((len(w2i), len(w2i)))\n",
    "\n",
    "    # for each sentence, go through all word pairs and update counts\n",
    "    for s in sentences_word_tok:\n",
    "        for i in range(len(s)):\n",
    "            x = s[i]\n",
    "            for j in range(i + 1, len(s)):\n",
    "                y = s[j]\n",
    "\n",
    "                counts[w2i[x], w2i[y]] = counts[w2i[x], w2i[y]] + 1\n",
    "\n",
    "    counts = counts + counts.T\n",
    "    return counts.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'naive sentence2counts'\n",
    "%time _ = sentences2counts_naive(sentences_word_tok, w2i)\n",
    "print\n",
    "print 'smarter sentence2counts'\n",
    "%time _ = sentences2counts(sentences_word_tok, w2i)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
